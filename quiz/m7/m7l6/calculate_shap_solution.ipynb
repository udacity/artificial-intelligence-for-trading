{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Shapley values (Solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapley values as used in coalition game theory were introduced by William Shapley in 1953.  \n",
    "[Scott Lundberg](http://scottlundberg.com/) applied Shapley values for calculating feature importance in [2017](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf).  \n",
    "\n",
    "If you want to read the paper, I recommend reading:  \n",
    "Abstract, 1 Introduction, 2 Additive Feature Attribution Methods, (skip 2.1, 2.2, 2.3), and 2.4 Classic Shapley Value Estimation.\n",
    "\n",
    "Lundberg calls this feature importance method \"SHAP\", which stands for SHapley Additive exPlanations.\n",
    "\n",
    "Here’s the formula for calculating Shapley values:\n",
    "\n",
    "$ \\phi_{i} = \\sum_{S \\subseteq M \\setminus i} \\frac{|S|! (|M| - |S| -1 )!}{|M|!} [f(S \\cup i) - f(S)]$\n",
    "\n",
    "A key part of this is the difference between the model’s prediction with the feature $i$, and the model’s prediction without feature $i$.  \n",
    "$S$ refers to a subset of features that doesn’t include the feature for which we're calculating $\\phi_i$.  \n",
    "$S \\cup i$ is the subset that includes features in $S$ plus feature $i$.  \n",
    "$S \\subseteq M \\setminus i$ in the $\\Sigma$ symbol is saying, all sets $S$ that are subsets of the full set of features $M$, excluding feature $i$.  \n",
    "\n",
    "##### Options for your learning journey\n",
    "* If you’re okay with just using this formula, you can skip ahead to the coding section below.   \n",
    "* If you would like an explanation for what this formula is doing, please continue reading here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional (explanation of this formula)\n",
    "\n",
    "The part of the formula with the factorials calculates the number of ways to generate the collection of features, where order matters.\n",
    "\n",
    "$\\frac{|S|! (|M| - |S| -1 )!}{|M|!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding features to a Coalition\n",
    "\n",
    "The following concepts come from coalition game theory, so when we say \"coalition\", think of it as a team, where members of the team are added, one after another, in a particular order.\n",
    "\n",
    "Let’s imagine that we’re creating a coalition of features, by adding one feature at a time to the coalition, and including all $|M|$ features.  Let’s say we have 3 features total.  Here are all the possible ways that we can create this “coalition” of features.\n",
    "\n",
    "<ol>\n",
    "    <li>$x_0,x_1,x_2$</li>\n",
    "    <li>$x_0,x_2,x_1$</li>\n",
    "    <li>$x_1,x_0,x_2$</li>\n",
    "    <li>$x_1,x_2,x_0$</li>\n",
    "    <li>$x_2,x_0,x_1$</li>\n",
    "    <li>$x_2,x_1,x_0$</li>\n",
    "</ol>\n",
    "\n",
    "Notice that for $|M| = 3$ features, there are $3! = 3 \\times 2 \\times 1 = 6$ possible ways to create the coalition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### marginal contribution of a feature\n",
    "\n",
    "For each of the 6 ways to create a coalition, let's see how to calculate the marginal contribution of feature $x_2$.\n",
    "\n",
    "<ol>\n",
    "<li>Model’s prediction when it includes features 0,1,2, minus the model’s prediction when it includes only features 0 and 1.  \n",
    "\n",
    "$x_0,x_1,x_2$: $f(x_0,x_1,x_2) - f(x_0,x_1)$  \n",
    "\n",
    "\n",
    "<li>Model’s prediction when it includes features 0 and 2, minus the prediction when using only feature 0.  Notice that feature 1 is added after feature 2, so it’s not included in the model.  \n",
    "    \n",
    "$x_0,x_2,x_1$: $f(x_0,x_2) - f(x_0)$</li>\n",
    "\n",
    "\n",
    "<li>Model's prediction including all three features, minus when the model is only given features 1 and 0.  \n",
    "\n",
    "$x_1,x_0,x_2$: $f(x_1,x_0,x_2) - f(x_1,x_0)$</li>\n",
    "\n",
    "\n",
    "<li>Model's prediction when given features 1 and 2, minus when the model is only given feature 1.  \n",
    "    \n",
    "$x_1,x_2,x_0$: $f(x_1,x_2) - f(x_1)$</li>\n",
    "\n",
    "\n",
    "<li>Model’s prediction if it only uses feature 2, minus the model’s prediction if it has no features.  When there are no features, the model’s prediction would be the average of the labels in the training data.  \n",
    "    \n",
    "$x_2,x_0,x_1$: $f(x_2) - f( )$\n",
    "</li>\n",
    "\n",
    "\n",
    "<li>Model's prediction (same as the previous one)  \n",
    "    \n",
    "$x_2,x_1,x_0$: $f(x_2) - f( )$\n",
    "</li>\n",
    "\n",
    "Notice that some of these marginal contribution calculations look the same.  For example the first and third sequences, $f(x_0,x_1,x_2) - f(x_0,x_1)$ would get the same result as $f(x_1,x_0,x_2) - f(x_1,x_0)$.  Same with the fifth and sixth.  So we can use factorials to help us calculate the number of permutations that result in the same marginal contribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### break into 2 parts\n",
    "\n",
    "To get to the formula that we saw above, we can break up the sequence into two sections: the sequence of features before adding feature $i$; and the sequence of features that are added after feature $i$.\n",
    "\n",
    "For the set of features that are added before feature $i$, we’ll call this set $S$.  For the set of features that are added after feature $i$ is added, we’ll call this $Q$.\n",
    "\n",
    "So, given the six sequences, and that feature $i$ is $x_2$ in this example, here’s what set $S$ and $Q$ are for each sequence:\n",
    "\n",
    "<ol>\n",
    "    <li>$x_0,x_1,x_2$: $S$ = {0,1}, $Q$ = {}</li>\n",
    "    <li>$x_0,x_2,x_1$: $S$ = {0},   $Q$ = {1}  </li>\n",
    "    <li>$x_1,x_0,x_2$: $S$ = {1,0}, $Q$ = {}  </li>\n",
    "    <li>$x_1,x_2,x_0$: $S$ = {1}, $Q$ = {0}  </li>\n",
    "    <li>$x_2,x_0,x_1$: $S$ = {}, $Q$ = {0,1}  </li>\n",
    "    <li>$x_2,x_1,x_0$: $S$ = {}, $Q$ = {1,0}  </li>\n",
    "</ol>\n",
    "So for the first and third sequences, these have the same set S = {0,1} and same set $Q$ = {}.  \n",
    "Another way to calculate that there are two of these sequences is to take $|S|! \\times |Q|! = 2! \\times 0! = 2$.\n",
    "\n",
    "Similarly, the fifth and sixth sequences have the same set S = {} and Q = {0,1}.  \n",
    "Another way to calculate that there are two of these sequences is to take $|S|! \\times |Q|! = 0! \\times 2! = 2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And now, the original formula\n",
    "\n",
    "To use the notation of the original formula, note that $|Q| = |M| - |S| - 1$.\n",
    "\n",
    "Recall that to calculate that there are 6 total sequences, we can use $|M|! = 3! = 3 \\times 2 \\times 1 = 6$.  \n",
    "We’ll divide $|S|! \\times (|M| - |S| - 1)!$ by $|M|!$ to get the proportion assigned to each marginal contribution.   \n",
    "This is the weight that will be applied to each marginal contribution, and the weights sum to 1.\n",
    "\n",
    "So that’s how we get the formula:  \n",
    "\n",
    "$\\frac{|S|! (|M| - |S| -1 )!}{|M|!} [f(S \\cup i) - f(S)]$  \n",
    "\n",
    "for each set $S \\subseteq M \\setminus i$\n",
    "\n",
    "We can sum up the weighted marginal contributions for all sets $S$, and this represents the importance of feature $i$.\n",
    "\n",
    "You’ll get to practice this in code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.14.5\n",
    "!{sys.executable} -m pip install scikit-learn==0.19.1\n",
    "!{sys.executable} -m pip install graphviz==0.9\n",
    "!{sys.executable} -m pip install shap==0.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import shap\n",
    "import numpy as np\n",
    "import graphviz\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate input data and fit a tree model\n",
    "We'll create data where features 0 and 1 form the \"AND\" operator, and feature 2 does not contribute to the prediction (because it's always zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND case (features 0 and 1)\n",
    "N = 100\n",
    "M = 3\n",
    "X = np.zeros((N,M))\n",
    "X.shape\n",
    "y = np.zeros(N)\n",
    "X[:1 * N//4, 1] = 1\n",
    "X[:N//2, 0] = 1\n",
    "X[N//2:3 * N//4, 1] = 1\n",
    "y[:1 * N//4] = 1\n",
    "\n",
    "# fit model\n",
    "model = sklearn.tree.DecisionTreeRegressor(random_state=0)\n",
    "model.fit(X, y)\n",
    "\n",
    "# draw model\n",
    "dot_data = sklearn.tree.export_graphviz(model, out_file=None, filled=True, rounded=True, special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Shap values\n",
    "\n",
    "We'll try to calculate the local feature importance of feature 0.  \n",
    "\n",
    "We have 3 features, $x_0, x_1, x_2$.  For feature $x_0$, determine what the model predicts with or without $x_0$.  \n",
    "\n",
    "Subsets S that exclude feature $x_0$ are:  \n",
    "{}  \n",
    "{$x_1$}  \n",
    "{$x_2$}  \n",
    "{$x_1,x_2$}  \n",
    "\n",
    "We want to see what the model predicts with feature $x_0$ compared to the model without feature $x_0$:  \n",
    "$f(x_0) - f( )$  \n",
    "$f(x_0,x_1) - f(x_1)$   \n",
    "$f(x_0,x_2) - f(x_2)$  \n",
    "$f(x_0,x_1,x_2) - f(x_1,x_2)$  \n",
    "\n",
    "## Sample data point\n",
    "We'll calculate the local feature importance of a sample data point, where  \n",
    "feature $x_0 = 1$   \n",
    "feature $x_1 = 1$  \n",
    "feature $x_2 = 1$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_values = np.array([1,1,1])\n",
    "print(f\"sample values to calculate local feature importance on: {sample_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function\n",
    "\n",
    "To make things easier, we'll use a helper function that takes the entire feature set M, and also a list of the features (columns) that we want, and puts them together into a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset(X, feature_l):\n",
    "    \"\"\"\n",
    "    Given a 2D array containing all feature columns,\n",
    "    and a list of integers representing which columns we want,\n",
    "    Return a 2D array with just the subset of features desired\n",
    "    \"\"\"\n",
    "    cols_l = []\n",
    "    for f in feature_l:\n",
    "        cols_l.append(X[:,f].reshape(-1,1))\n",
    "        \n",
    "    return np.concatenate(cols_l, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out\n",
    "tmp = get_subset(X,[0,2])\n",
    "tmp[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function to calculate permutation weight\n",
    "\n",
    "This helper function calculates  \n",
    "\n",
    "$\\frac{|S|! (|M| - |S| - 1)!}{|M|!}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import factorial\n",
    "def calc_weight(size_S, num_features):\n",
    "    return factorial(size_S) * factorial(num_features - size_S - 1) / factorial(num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out when size of S is 2 and there are 3 features total.  \n",
    "The answer should be equal to $\\frac{2! \\times (3-2-1)!}{3!} = \\frac{2 \\times 1}{6} = \\frac{1}{3}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_weight(size_S=2,num_features=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## case A  \n",
    "\n",
    "Calculate the prediction of a model that uses features 0 and 1  \n",
    "Calculate the prediction of a model that uses feature 1  \n",
    "Calculate the difference (the marginal contribution of feature 0)\n",
    "\n",
    "$f(x_0,x_1) - f(x_1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0,x_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S_union_i\n",
    "S_union_i = get_subset(X,[0,1])\n",
    "\n",
    "# fit model\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, for the sample input for which we'll calculate feature importance, we chose values of 1 for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will throw an error\n",
    "\n",
    "try:\n",
    "    f_S_union_i.predict(np.array([1,1]))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error message says:\n",
    "\n",
    ">Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.\n",
    "\n",
    "So we'll reshape the data so that it represents a sample (a row), which means it has 1 row and 1 or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature 0 and feature 1 are both 1 in the sample input\n",
    "sample_input = np.array([1,1]).reshape(1,-1)\n",
    "sample_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction of the model when it has features 0 and 1 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When feature 0 and feature 1 are both 1, the prediction of the model is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S\n",
    "S = get_subset(X,[1])\n",
    "f_S = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S.fit(S, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample input for feature 1 is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = np.array([1]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's prediction when it is only training on feature 1 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_S = f_S.predict(sample_input)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When feature 1 is 1, then the prediction of this model is 0.5.  If you look at the data in X, this makes sense, because when feature 1 is 1, half of the time, the label in y is 0, and half the time, the label in y is 1. So on average, the prediction is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_A = pred_S_union_i - pred_S\n",
    "diff_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the weight\n",
    "Calculate the weight assigned to the marginal contribution.  In this case, if this marginal contribution occurs 1 out of the 6 possible permutations of the 3 features, then its weight is 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_S = S.shape[1] # should be 1\n",
    "weight_A = calc_weight(size_S, M)\n",
    "weight_A # should be 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Case B\n",
    "\n",
    "Calculate the prediction of a model that uses features 0 and 2  \n",
    "Calculate the prediction of a model that uses feature 2  \n",
    "Calculate the difference\n",
    "\n",
    "$f(x_0,x_2) - f(x_2)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0,x_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S_union_i = get_subset(X,[0,2])\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)\n",
    "\n",
    "sample_input = np.array([1,1]).reshape(1,-1)\n",
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using features 0 and 2, and feature 2 doesn't help with predicting the output, then the model really just depends on feature 0.  When feature 0 is 1, half of the labels are 0, and half of the labels are 1.  So the average prediction is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_2)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S = get_subset(X,[2])\n",
    "f_S = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S.fit(S, y)\n",
    "\n",
    "sample_input = np.array([1]).reshape(1,-1)\n",
    "pred_S = f_S.predict(sample_input)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since feature 2 doesn't help with predicting the labels in y, and feature 2 is 0 for all 100 training observations, then the prediction of the model is the average of all 100 training labels.  1/4 of the labels are 1, and the rest are 0.  So that prediction is 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the difference in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "diff_B = pred_S_union_i - pred_S\n",
    "diff_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "size_S = S.shape[1] # is 1\n",
    "weight_B = calc_weight(size_S, M)\n",
    "weight_B # should be 1/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz: Case C\n",
    "\n",
    "Calculate the prediction of a model that uses features 0,1 and 2  \n",
    "Calculate the prediction of a model that uses feature 1 and 2  \n",
    "Calculate the difference\n",
    "\n",
    "$f(x_0,x_1,x_2) - f(x_1,x_2)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0,x_1,x_2) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S_union_i = get_subset(X,[0,1,2])\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)\n",
    "\n",
    "sample_input = np.array([1,1,1]).reshape(1,-1)\n",
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use all three features, the model is able to predict that if feature 0 and feature 1 are both 1, then the label is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_1,x_2)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S = get_subset(X,[1,2])\n",
    "f_S = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S.fit(S, y)\n",
    "\n",
    "sample_input = np.array([1,1]).reshape(1,-1)\n",
    "pred_S = f_S.predict(sample_input)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is trained on features 1 and 2, then its training data tells it that half of the time, when feature 1 is 1, the label is 0; and half the time, the label is 1.  So the average prediction of the model is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate difference in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "diff_C = pred_S_union_i - pred_S\n",
    "diff_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "size_S = S.shape[1]\n",
    "weight_C = calc_weight(size_S,M) # should be 2 / 6 = 1/3\n",
    "weight_C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: case D: remember to include the empty set!\n",
    "\n",
    "The empty set is also a set.  We'll compare how the model does when it has no features, and see how that compares to when it gets feature 0 as input.\n",
    "\n",
    "Calculate the prediction of a model that uses features 0.  \n",
    "Calculate the prediction of a model that uses no features \n",
    "Calculate the difference\n",
    "\n",
    "$f(x_0) - f()$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f(x_0)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "S_union_i = get_subset(X,[0])\n",
    "f_S_union_i = sklearn.tree.DecisionTreeRegressor()\n",
    "f_S_union_i.fit(S_union_i, y)\n",
    "\n",
    "sample_input = np.array([1]).reshape(1,-1)\n",
    "pred_S_union_i = f_S_union_i.predict(sample_input)\n",
    "pred_S_union_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just feature 0 as input, the model predicts 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate $f()$\n",
    "**hint**: you don't have to fit a model, since there are no features to input into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# with no input features, the model will predict the average of the labels, which is 0.25\n",
    "pred_S = np.mean(y)\n",
    "pred_S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With no input features, the model's best guess is the average of the labels, which is 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate difference in predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "diff_D = pred_S_union_i - pred_S\n",
    "diff_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate weight\n",
    "\n",
    "We expect this to be: 0! * (3-0-1)! / 3! = 2/6 = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "size_S = 0\n",
    "weight_D = calc_weight(size_S,M) # weight is 1/3\n",
    "weight_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Shapley value\n",
    "For a single sample observation, where feature 0 is 1, feature 1 is 1, and feature 2 is 1, calculate the shapley value of feature 0 as the weighted sum of the differences in predictions.\n",
    "\n",
    "$\\phi_{i} = \\sum_{S \\subseteq N \\setminus i} weight_S \\times (f(S \\cup i) - f(S))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "shap_0 = weight_A * diff_A + weight_B * diff_B + weight_C * diff_C + weight_D * diff_D\n",
    "shap_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify with the shap library\n",
    "\n",
    "The [shap](https://github.com/slundberg/shap) library is written by Scott Lundberg, the creator of Shapley Additive Explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_values = np.array([1,1,1])\n",
    "shap_values = shap.TreeExplainer(model).shap_values(sample_values)\n",
    "\n",
    "print(f\"Shapley value for feature 0 that we calculated: {shap_0}\")\n",
    "print(f\"Shapley value for feature 0 is {shap_values[0]}\")\n",
    "print(f\"Shapley value for feature 1 is {shap_values[1]}\")\n",
    "print(f\"Shapley value for feature 2 is {shap_values[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Does this make sense?\n",
    "\n",
    "The shap libary outputs the shap values for features 0, 1 and 2.  We can see that the shapley value for feature 0 matches what we calculated.  The Shapley value for feature 1 is also given the same importance as feature 0.  \n",
    "* Given that the training data is simulating an AND operation, do you think these values make sense?  \n",
    "* Do you think feature 0 and 1 are equally important, or is one more important than the other?  \n",
    "* Does the importane of feature 2 make sense as well?\n",
    "* How does this compare to the feature importance that's built into sci-kit learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "* The shapley values appear to make sense, since feature 0 and feature 1 are both required to be 1 for the label to also be 1.  So it makes sense that their importance values are equal.  \n",
    "* Features 0 and 1 are equally important, based on our intuition about the AND operator, and the shapley values calculated by the shap library.\n",
    "* Since feature 2 does not help in predicting the label, it also makes sense that the importance of feature 2 is 0.\n",
    "* Compared to the feature importance method built into sci-kit learn, the shapley additive explanations method is more consistent in assigning the importance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "This method is general enough that it works for any model, not just trees.  There is an optimized way to calculate this when the complex model being explained is a tree-based model.  We'll look at that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "[Solution notebook](calculate_shap_solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "env_p7a",
   "language": "python",
   "name": "env_p7a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
